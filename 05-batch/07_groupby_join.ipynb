{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "299665b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/05 09:33:55 WARN Utils: Your hostname, MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 100.112.89.122 instead (on interface en0)\n",
      "25/03/05 09:33:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/05 09:33:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e735457c",
   "metadata": {},
   "source": [
    "# GroupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9a655b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green = spark.read.parquet('data/pq/green/*/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed13452c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/pyspark/sql/dataframe.py:329: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "df_green.registerTempTable('green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "727cff6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green_revenue = spark.sql(\"\"\"\n",
    "SELECT \n",
    "  date_trunc('hour', lpep_pickup_datetime) AS hour\n",
    ", PULocationID AS zone \n",
    ", SUM(total_amount) AS amount\n",
    ", COUNT(1) AS number_records\n",
    "FROM green \n",
    "WHERE 1=1\n",
    "      AND lpep_pickup_datetime >= '2020-01-01'\n",
    "GROUP BY 1,2\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1886d17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+------------------+--------------+\n",
      "|               hour|zone|            amount|number_records|\n",
      "+-------------------+----+------------------+--------------+\n",
      "|2020-01-11 14:00:00|  42|247.86000000000004|            19|\n",
      "|2020-01-07 20:00:00|  92|              80.6|             5|\n",
      "|2020-01-28 16:00:00| 223|            241.33|            15|\n",
      "|2020-01-12 21:00:00| 181| 325.9000000000001|            19|\n",
      "|2020-01-27 18:00:00| 165|             32.11|             1|\n",
      "|2020-01-07 17:00:00| 126|            131.31|             4|\n",
      "|2020-01-04 10:00:00|  69|232.78000000000003|             7|\n",
      "|2020-01-29 16:00:00| 133|             47.46|             2|\n",
      "|2020-01-19 10:00:00| 205| 64.25999999999999|             3|\n",
      "|2020-01-08 14:00:00| 140|            212.39|             6|\n",
      "|2020-01-18 22:00:00|  49|             24.68|             2|\n",
      "|2020-01-31 22:00:00| 181|            648.09|            30|\n",
      "|2020-01-25 18:00:00|  95|229.65000000000006|            18|\n",
      "|2020-01-10 17:00:00|  49|            206.28|            11|\n",
      "|2020-01-28 12:00:00| 129|             147.9|            10|\n",
      "|2020-01-09 20:00:00|  95| 394.0200000000002|            37|\n",
      "|2020-01-15 19:00:00|  74| 837.2999999999994|            63|\n",
      "|2020-01-03 16:00:00|  55|            370.18|             8|\n",
      "|2020-01-19 16:00:00|  41| 668.6899999999998|            52|\n",
      "|2020-01-04 12:00:00|  25|            144.27|            10|\n",
      "+-------------------+----+------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_green_revenue.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd3241b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/05 09:41:01 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/03/05 09:41:01 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/03/05 09:41:01 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/03/05 09:41:01 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "25/03/05 09:41:01 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/03/05 09:41:01 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/03/05 09:41:01 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    }
   ],
   "source": [
    "df_green_revenue. \\\n",
    "    .repartition(20) \\\n",
    "    .write.parquet('data/report/revenue/green', mode = 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82207275",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow = spark.read.parquet('data/pq/yellow/*/*')\n",
    "df_yellow.registerTempTable('yellow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1afca96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow_revenue = spark.sql(\"\"\"\n",
    "SELECT \n",
    "  date_trunc('hour', tpep_pickup_datetime) AS hour\n",
    ", PULocationID AS zone \n",
    ", SUM(total_amount) AS amount\n",
    ", COUNT(1) AS number_records\n",
    "FROM yellow \n",
    "WHERE 1=1\n",
    "      AND tpep_pickup_datetime >= '2020-01-01'\n",
    "GROUP BY 1,2\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69d8a20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/05 09:52:33 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/03/05 09:52:33 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/03/05 09:52:33 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/03/05 09:52:33 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "25/03/05 09:52:33 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/03/05 09:52:33 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/03/05 09:52:33 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/03/05 09:52:33 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/03/05 09:52:33 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/03/05 09:52:33 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_yellow_revenue \\\n",
    "    .repartition(20) \\\n",
    "    .write.parquet('data/report/revenue/yellow', mode = 'overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb26c66",
   "metadata": {},
   "source": [
    "# Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c9d695",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2612007",
   "metadata": {},
   "source": [
    "Let's materialize our results for more efficient processing (without this next bit of code, we'd be recomputing everything each time we run!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cbbe7bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## USING PREVIOUS RESULTS!\n",
    "df_green_revenue = spark.read.parquet('data/report/revenue/green')\n",
    "df_yellow_revenue = spark.read.parquet('data/report/revenue/yellow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "50d041af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green_revenue_tmp = df_green_revenue \\\n",
    "    .withColumnRenamed('amount', 'green_amount') \\\n",
    "    .withColumnRenamed('number_records', 'green_number_records') \n",
    "\n",
    "df_yellow_revenue_tmp = df_yellow_revenue \\\n",
    "    .withColumnRenamed('amount', 'yellow_amount') \\\n",
    "    .withColumnRenamed('number_records', 'yellow_number_records') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a5e57b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join = df_green_revenue_tmp.join(df_yellow_revenue_tmp, on = ['hour', 'zone'], how = 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a10da447",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/05 10:12:11 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/03/05 10:12:11 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/03/05 10:12:11 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/03/05 10:12:11 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "25/03/05 10:12:12 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/03/05 10:12:12 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/03/05 10:12:12 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_join.write.parquet('data/report/revenue/total', mode = 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d4dcb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 48:===============================================>        (11 + 2) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+------------------+--------------------+------------------+---------------------+\n",
      "|               hour|zone|      green_amount|green_number_records|     yellow_amount|yellow_number_records|\n",
      "+-------------------+----+------------------+--------------------+------------------+---------------------+\n",
      "|2020-01-01 00:00:00|  36|295.34000000000003|                  11|            109.17|                    3|\n",
      "|2020-01-01 00:00:00|  45|              NULL|                NULL| 732.4800000000002|                   42|\n",
      "|2020-01-01 00:00:00|  59|50.900000000000006|                   3|              NULL|                 NULL|\n",
      "|2020-01-01 00:00:00|  76|            143.78|                   4|             35.51|                    2|\n",
      "|2020-01-01 00:00:00|  79|              NULL|                NULL|12573.810000000034|                  721|\n",
      "|2020-01-01 00:00:00|  90|              NULL|                NULL| 5010.450000000003|                  266|\n",
      "|2020-01-01 00:00:00|  92|             80.72|                   3|              75.3|                    1|\n",
      "|2020-01-01 00:00:00| 114|              NULL|                NULL| 6256.430000000006|                  333|\n",
      "|2020-01-01 00:00:00| 148|              NULL|                NULL| 6790.150000000011|                  371|\n",
      "|2020-01-01 00:00:00| 190|             61.97|                   4|54.099999999999994|                    3|\n",
      "|2020-01-01 00:00:00| 191|             84.21|                   2|              57.3|                    1|\n",
      "|2020-01-01 00:00:00| 208|             80.24|                   3|              NULL|                 NULL|\n",
      "|2020-01-01 00:00:00| 258|              45.3|                   1|              NULL|                 NULL|\n",
      "|2020-01-01 00:00:00| 259|131.23999999999998|                   7|              NULL|                 NULL|\n",
      "|2020-01-01 01:00:00|  17| 598.1500000000001|                  18|464.51000000000005|                   18|\n",
      "|2020-01-01 01:00:00|  18|              NULL|                NULL|              36.0|                    1|\n",
      "|2020-01-01 01:00:00|  34|              NULL|                NULL|             19.89|                    1|\n",
      "|2020-01-01 01:00:00|  50|              NULL|                NULL| 6065.200000000004|                  279|\n",
      "|2020-01-01 01:00:00|  53|              4.94|                   1|              NULL|                 NULL|\n",
      "|2020-01-01 01:00:00|  67|              NULL|                NULL|              17.8|                    1|\n",
      "+-------------------+----+------------------+--------------------+------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_join.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffdd7b9",
   "metadata": {},
   "source": [
    "## Broadcast Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2a03a2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones = spark.read.parquet('data/zones/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9fd683f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "|         6|Staten Island|Arrochar/Fort Wad...|   Boro Zone|\n",
      "|         7|       Queens|             Astoria|   Boro Zone|\n",
      "|         8|       Queens|        Astoria Park|   Boro Zone|\n",
      "|         9|       Queens|          Auburndale|   Boro Zone|\n",
      "|        10|       Queens|        Baisley Park|   Boro Zone|\n",
      "|        11|     Brooklyn|          Bath Beach|   Boro Zone|\n",
      "|        12|    Manhattan|        Battery Park| Yellow Zone|\n",
      "|        13|    Manhattan|   Battery Park City| Yellow Zone|\n",
      "|        14|     Brooklyn|           Bay Ridge|   Boro Zone|\n",
      "|        15|       Queens|Bay Terrace/Fort ...|   Boro Zone|\n",
      "|        16|       Queens|             Bayside|   Boro Zone|\n",
      "|        17|     Brooklyn|             Bedford|   Boro Zone|\n",
      "|        18|        Bronx|        Bedford Park|   Boro Zone|\n",
      "|        19|       Queens|           Bellerose|   Boro Zone|\n",
      "|        20|        Bronx|             Belmont|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_zones.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f4eb6dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = df_join.join(df_zones, df_join.zone == df_zones.LocationID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f863dcfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/05 10:21:44 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/03/05 10:21:44 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/03/05 10:21:44 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/03/05 10:21:44 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "25/03/05 10:21:44 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/03/05 10:21:44 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/03/05 10:21:44 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# df_result.drop('LocationID').show()\n",
    "df_result.drop('LocationID', 'zone').write.parquet('tmp/revenue_zones', mode = 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6da52f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36e4b6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
