{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b69db81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/05 17:02:09 WARN Utils: Your hostname, MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.36.156.69 instead (on interface en0)\n",
      "25/03/05 17:02:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/05 17:02:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aba3648",
   "metadata": {},
   "source": [
    "### Build RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6024fb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green = spark.read.parquet('data/pq/green/*/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3afe9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = df_green \\\n",
    "    .select('lpep_pickup_datetime', 'PULocationID', 'total_amount') \\\n",
    "    .rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93bb0912",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime \n",
    "start = datetime(year = 2020, month = 1, day = 1)\n",
    "\n",
    "# Filter\n",
    "def filter_outliers(row):\n",
    "    return row.lpep_pickup_datetime >= start\n",
    "\n",
    "# Map\n",
    "def prepare_for_grouping(row): \n",
    "    hour = row.lpep_pickup_datetime.replace(minute = 0, second = 0, microsecond = 0)\n",
    "    zone = row.PULocationID\n",
    "    \n",
    "    amount = row.total_amount\n",
    "    count = 1 \n",
    "    \n",
    "    # Establish key-value pair for groupBy later\n",
    "    key = (hour, zone)\n",
    "    value = (amount, count)\n",
    "    \n",
    "    return (key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c1408b",
   "metadata": {},
   "source": [
    "### Reduce\n",
    "We are going to *reduce by key*, generating a `(key, reduced_value)` pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56fa5756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_revenue(left_value, right_value): \n",
    "    \n",
    "    # Unpack tuple\n",
    "    left_amount, left_count = left_value\n",
    "    right_amount, right_count = right_value\n",
    "    \n",
    "    output_amount = left_amount + right_amount\n",
    "    output_count = left_count + right_count\n",
    "    \n",
    "    output_value = (output_amount, output_count)\n",
    "    \n",
    "    return output_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda3a18b",
   "metadata": {},
   "source": [
    "### Unwrap\n",
    "Unwrapping the aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b7054ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "RevenueRow = namedtuple('RevenueRow', ['hour', 'zone', 'revenue', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2de5c7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unwrap(row): \n",
    "    return RevenueRow(\n",
    "        hour = row[0][0], \n",
    "        zone = row[0][1], \n",
    "        revenue = row[1][0], \n",
    "        count = row[1][1]\n",
    "    )\n",
    "\n",
    "# Without namedtuple, the resultant DataFrame wouldn't have column names, \n",
    "# as RDDs inherently don't use them!\n",
    "# We will also generate a more specific schema and feed that to `.toDF()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9183000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n",
    "\n",
    "result_schema = types.StructType([\n",
    "    types.StructField('hour', types.TimestampType(), True),\n",
    "    types.StructField('zone', types.IntegerType(), True),\n",
    "    types.StructField('revenue', types.DoubleType(), True),\n",
    "    types.StructField('count', types.IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0022adae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_result = rdd \\\n",
    "    .filter(filter_outliers) \\\n",
    "    .map(prepare_for_grouping) \\\n",
    "    .reduceByKey(calculate_revenue) \\\n",
    "    .map(unwrap) \\\n",
    "    .toDF(result_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cab128f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_result.write.parquet('tmp/green-revenue')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0e2636",
   "metadata": {},
   "source": [
    "This effectively reproduces what we did in `06_spark_sql.ipynb`!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16fef0c",
   "metadata": {},
   "source": [
    "## `mapPartitions()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f81c83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['VendorID', 'lpep_pickup_datetime', 'PULocationID', 'DOLocationID', 'trip_distance']\n",
    "\n",
    "duration_rdd = df_green \\\n",
    "    .select(cols) \\\n",
    "    .rdd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61186016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a9c4a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ...\n",
    "\n",
    "def model_predict(df): \n",
    "    \n",
    "#     # FAKE model predictions...\n",
    "#     y_pred = model.predict(df)\n",
    "    y_pred = df.trip_distance * 5\n",
    "    \n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0474577b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def apply_model_in_batch(rows):\n",
    "    \n",
    "    df = pd.DataFrame(rows, columns = cols)\n",
    "    predictions = model_predict(df)\n",
    "    \n",
    "    df['predicted_duration'] = predictions\n",
    "    \n",
    "    for row in df.itertuples():\n",
    "        \n",
    "        yield row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "762f085f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# `.collect()` collects all elements of the RDD and returns a list\n",
    "\n",
    "df_predicts = duration_rdd \\\n",
    "    .mapPartitions(apply_model_in_batch)\\\n",
    "    .toDF() \\\n",
    "    .drop('Index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f47aa68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|predicted_duration|\n",
      "+------------------+\n",
      "| 4.050000000000001|\n",
      "|              15.1|\n",
      "|6.6000000000000005|\n",
      "|              4.45|\n",
      "|             14.15|\n",
      "|              6.65|\n",
      "|             17.05|\n",
      "|               7.5|\n",
      "|               1.0|\n",
      "|12.350000000000001|\n",
      "|              97.6|\n",
      "|               2.6|\n",
      "|              40.9|\n",
      "|             14.75|\n",
      "|               7.9|\n",
      "|               4.3|\n",
      "|              79.2|\n",
      "| 6.550000000000001|\n",
      "|3.5999999999999996|\n",
      "|               5.5|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_predicts.select('predicted_duration').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d476995",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08bbcec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
